{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d30d65c",
   "metadata": {},
   "source": [
    "# Universitat Oberta de Catalunya  \n",
    "## Grado en Ingeniería Informática  \n",
    "### Trabajo Final de Grado (TFG)\n",
    "\n",
    "---\n",
    "\n",
    "## Sistema de recomendaciones basado en técnicas de aprendizaje automático para ampliar la exploración de géneros musicales \n",
    "**Autor:** Marc Fernández Pereira  \n",
    "**Bajo supervisión de:** Dra. María Moreno de Castro  \n",
    "**Área:** Inteligencia Artificial  \n",
    "**Semestre:** Otoño 2025  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e728c0c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "# 3. Fase de modelado\n",
    "\n",
    "\n",
    "## 3.1 Introducción\n",
    "\n",
    "El objetivo de esta fase es abordar tanto el desarrollo como la evaluación del modelo mediante distintas técnicas de aprendizaje no supervisado. Después de llevar a cabo un análisis exploratorio del conjunto de datos y de preparar el dataset con el que se trabajará, se procede a aplicar los algoritmos de *clustering* seleccionados con el fin de identificar patrones de similitud entre canciones basados en sus características musicales. Para cada resultado obtenido con cada uno de los modelo se redactará una breve descripción que permitirá al lector comprender los resultados sin necesidad de dominar los fundamentos básicos de las técnicas del aprendizaje profundo (*deep learning*). Al finalizar la tarea de modelado se elaborará un análisis exhaustivo del rendimiento comparado de los modelos, empleando como criterios los índices de Silhouette, Davies–Bouldin y Calinski–Harabasz (véase apartado 3.3).\n",
    "\n",
    "## 3.2 Selección de las técnicas de modelado\n",
    "\n",
    "A continuación se propone el conjunto de técnicas de aprendizaje no supervisado que se abordarán en este proyecto:\n",
    "\n",
    "### 3.2.1 K-Means\n",
    "\n",
    "El algoritmo K-Means es un proceso iterativo que busca particionar el conjunto de datos en $k$ grupos con el objetivo minimizar la suma de distancias cuadráticas entre los puntos y los centroides de cada grupo formado. En cada iteración, las referencias de los centroides se actualizan con la media de los puntos asignados hasta que el proceso converge dando lugar a grupos estabilizados.\n",
    "\n",
    "Para establecer el valor $k$, habitualmente se utilizan diferentes técnicas como el método del codo (*Elbow method*), el criterio de la silueta (*Silhoutte Score*) o el índice de Calinski-Harabasz. En este proyecto se decide aplicar las tres técnicas con el objetivo de visualizar **cómo varía la calidad de agrupamiento** en función del número de clústeres y determinar el valor de $k$ que ofrezca el mejor equilibrio entre los grupos.\n",
    "\n",
    "En algoritmo K-Means funciona de la siguiente manera:\n",
    "\n",
    "1. En primer lugar se seleccionan el número de clústeres $k$ utilizando las técnicas anteriores. Que estas técnicas ofrezcan un valor $k$ determinado no implica tener que utilizar únicamente dicho valor, es importante destacar que se pueden generar agrupaciones entorno al valor $k$ que sugieren las técnicas.\n",
    "\n",
    "2. Acto seguido se inicializan las coordenadas de los centroides. Esto se puede realizar de manera aleatoria o simplemente indicando el valor inicial de manera manual. \n",
    "\n",
    "3. Se procede con la asignación de puntos a cada clúster utilizando una métrica de distancia como la euclidiana o la manhattan entre otros.\n",
    "\n",
    "4. Una vez se tienen los puntos del espacio asignados a un clúster determinado, se actualizan los centroides con la siguiente expresión:  \n",
    "    $$\n",
    "    \\mathbf{m}_i^{(t+1)} = \\frac{1}{|S_i^{(t)}|} \\sum_{\\mathbf{x}_j \\in S_i^{(t)}} \\mathbf{x}_j\n",
    "    $$  \n",
    "    donde:\n",
    "    - $\\mathbf{m}_i^{(t+1)}$: El nuevo centroide del clúster $i$ en la iteración $t + 1$\n",
    "    - $|S_i^{(t)}|$: Número de puntos pertenecientes al clúster $i$ en la iteración $t$ \n",
    "    - $x_j$: Representa cada punto del clúster \n",
    "\n",
    "5. Se repiten de manera iterativa los pasos 3 y 4 hasta que el algoritmo converge.\n",
    "\n",
    "Para que el algoritmo converja debe ocurrir que los centroides dejen de cambiar de clúster, llegados a este punto se tienen formado los clústeres que k-Means identifica en el conjunto de datos. \n",
    "\n",
    "\n",
    "### 3.2.2 K-Medoids\n",
    "\n",
    "K-Medoids también conocido como Partitioning Around Medoids (PAM) es un algoritmo de agrupamiento que se utiliza para dividir un conjunto de datos en $k$ grupos. A diferencia de K-Means donde se utiliza el cálculo de medias para establecer los centroides en cada iteración, en K-Medoids se utilizan puntos reales del conjunto de datos bajo el nombre de *medoids*. Este algoritmo es realmente eficiente sobre conjuntos de datos que presentan valores atípicos ya que los centroides se eligen como puntos reales del conjunto de datos en lugar de utilizarse las medias.\n",
    "\n",
    "Para realizar el agrupamiento de los datos a los medoids se utiliza el enfoque de PAM:\n",
    "\n",
    "1. Al igual que ocurre con k-Means se selecciona un valor $k$ utilizando las técnicas expuestas anteriormente.\n",
    "\n",
    "2. Se inicializa aleatoriamente los medoides eligiendo $k$ puntos reales del conjunto de datos.\n",
    "\n",
    "3. Se utiliza una métrica de distancia entre cada dato a los distintos medoides para agruparlo al medoide más cercano.\n",
    "\n",
    "4. Para cada clúster y una vez se realizan todas las asignaciones, se debe comprobar si alguno de los puntos del reduce el coeficiente de disimilitud, es decir, la suma de distancias dentro del grupo. Si se encuentra \n",
    "algún punto que cumpla esta condición se convertirá automáticamente en el nuevo medoide. \n",
    "\n",
    "5. Solamente que un medoide cambie, se realizará el proceso de nuevo.\n",
    "\n",
    "El algoritmo converge cuando todos los medoides no cambian en dos iteraciones seguidas. \n",
    "\n",
    "\n",
    "### 3.2.3 DBSCAN y OPTICS\n",
    "\n",
    "*Density-Based Spatial Clustering of Applications with Noise* (DBSCAN) y *Ordering Points To Identify The Clustering Structure*(OPTICS) son métodos de clustering no supervisado en que la agrupación está basada en la densidad de los datos. El método DBSCAN permite identificar patrones en un juego de datos agrupando los puntos que están más cercanos a alguna métrica como la distancia euclidiana o la manhattan. El método recibe dos parámetros, eps y minPoints:\n",
    "\n",
    "- **epsilon (eps)** : distancia máxima entre dos puntos para que uno sea considerado vecino del otro\n",
    "\n",
    "- **minPoints**: Establece el mínimo de puntos para que el método lo considere un clúster\n",
    "\n",
    "DBSCAN tiene la limitación de que trabaja con agrupación de densidades de datos fijas. El método OPTICS, podría considerarse una extensión de DBSCAN que ofrece la oportunidad de trabajar con agrupación de densidades variables. Además, el parámetro epsilon en OPTICS no determina la formación de clústeres sino que sirve para disminuir la complejidad de cálculo para el algoritmo.\n",
    "\n",
    "\n",
    "## 3.3 Generar un diseño de comprobación.\n",
    "\n",
    "Una vez seleccionadas las técnicas de modelado que pueden encajar con el proyecto, es crucial fijar una estrategia de verificación de la calidad del modelo que permita evaluar los resultados obtenidos. La estrategia debe estar enfocada en cómo se evaluarán los resultados con el objetivo no solo de medir el rendimiento, sino también para comprender el modelo a un nivel más profundo. El esquema general de validación estará compuesto por lo siguiente:\n",
    "\n",
    "### 3.3.1 Validación interna\n",
    "\n",
    "Una diferencia principal entre los tipos de aprendizaje automático es que el aprendizaje no supervisado no utiliza etiquetas de referencia para generar las agrupaciones. Dado este hecho, existen métricas de rendimiento que permiten evaluar características como la cohesión de las muestras dentro de cada clúster y la separación entre los distintos grupos. A continuación, se proponen tres métricas recomendadas por Montoliu (2021) en Evaluación de modelos no supervisados [31, ver memoria bibliografía del proyecto] para la validación interna de algoritmos no supervisados:\n",
    "\n",
    "**Coeficiente de Silhouette**\n",
    "\t\n",
    "El coeficiente intenta describir la similitud entre una muestra y el resto de las muestras de un grupo con los de otro grupo distinto. El resultado de esta métrica se expresa en un rango de [-1, 1], donde 1 indica que la muestra está bien asignada y -1 indica que la muestra está mal asignada. Para obtener una medida global de la calidad del agrupamiento, se calcula el promedio de los coeficientes de Silhouette de todas las muestras del conjunto de datos:\n",
    "\n",
    "$$\n",
    "SC = \\frac{1}{N} \\sum_{i=1}^{N} s(x_i)\n",
    "$$\n",
    "\n",
    "donde $N$ es el número total de observaciones y $s(x_i)$ representa el coeficiente de Silhouette individual de cada muestra, definido como:\n",
    "\n",
    "$$\n",
    "s(x_i) = \\frac{b(x_i) - a(x_i)}{\\max(a(x_i), b(x_i))}\n",
    "$$\n",
    "\n",
    "donde: \n",
    "- $a(x_i)$ es la medida de cohesión, es decvir, la distancia promedio de una muestra a todas las observaciones de su mismo grupo.\n",
    " - $b(x_i)$ es la medida de separación, es decir, la distancia mínima promedio de esa muestra a cualquier otro grupo.  \n",
    "\n",
    "Cuanto mayor sea el valor de $SC$ reflejará alta cohesión interna y buena separación entre grupos lo que se traduce en una mejor será la estructura de los clústeres.\n",
    "\n",
    "\n",
    "**Índice de Davies-Bouldin (DB)**\n",
    "\n",
    "Este índice se basa en medir la relación entre la dispersión intra-clúster y la separación entre clústeres. Valores bajos indican que los clústeres son más compactos cuyos centroides están bien separados los unos de los otros. Se puede calcular mediante la siguiente ecuación:\n",
    "\n",
    "$$\n",
    "DB = \\frac{1}{k} \\sum_{i=1}^{k} \\max_{j \\neq i} \\left( \\frac{S_i + S_j}{d_{ij}} \\right)\n",
    "$$\n",
    "\n",
    "donde $d_{ij}$ es la distancia entre los centroides de los grupos $G_i$ y $G_j$,  \n",
    "$S_i$ es la distancia promedio entre cada punto del grupo $G_i$ y su centroide $\\mu_i$,  \n",
    "y $S_j$ es la distancia promedio entre cada punto del grupo $G_j$ y su centroide $\\mu_j$.  \n",
    "\n",
    "Cuanto más bajo sea el valor del índice $DB$, más compactos y mejor separados estarán los clústeres.\n",
    "\n",
    "\n",
    "**Índice de Calínski-Haranaz(CH)**\n",
    "\n",
    "Para obtener este índice se utilizan los índices basados SSW (Sum of squared within) y SSB (Sum of squared between). El SSW se utiliza para medir la cohesión interna de los grupos obtenidos y SSB se emplea para medir la separación entre los grupos obtenidos. Las ecuaciones que definen a estos índices son las siguientes.\n",
    "\n",
    "$$\n",
    "SSW = \\sum_{i=1}^{k} \\sum_{x_j \\in G_i} (x_j - \\mu_i)^2\n",
    "$$\n",
    "\n",
    "donde $k$ es el número de clústeres, $x_j$ una muestra del grupo $G_i$ y $\\mu_i$ es el centroide del $i$-ésimo grupo $G_i$.\n",
    "\n",
    "$$\n",
    "SSB = \\sum_{i=1}^{k} |G_i| (\\mu_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "donde $k$ es el número de clústeres, $|G_i|$ es el número de muestras del grupo $G_i$, $\\mu_i$ es el centroide del $i$-ésimo grupo $G_i$ y $\\mu$ es la media de todo el conjunto de datos.\n",
    "\n",
    "\n",
    "Una vez se obtienen los dos índices se podrá obtener el índice CH:\n",
    "\n",
    "\n",
    "$$\n",
    "CH = \\frac{SSB / (k - 1)}{SSW / (N - k)}\n",
    "$$\n",
    "\n",
    "donde $k$ es el número de clústeres y $N$ el número total de observaciones.  \n",
    "Por lo tanto, cuanto mayor sea el valor de $CH$, mejor reflejará el equilibrio entre una **alta separación entre clústeres** ($SSB$) y una **baja dispersión interna** ($SSW$).\n",
    "\n",
    "\n",
    "### 3.3.2 Validación externa\n",
    "\n",
    "En este proyecto no se emplea la etiqueta de género musical como variable de entrada en el proceso de clustering, ya que el objetivo es descubrir patrones de similitud entre canciones independientemente del género al que pertenecen. Sin embargo, este campo se podría utilizar como etiqueta externa para comparar la similitud entre diferentes métodos de clustering mediante el Índice de Rand Ajustado (ARI) [30]. \n",
    "El ARI intenta expresar qué proporción de asignaciones del clúster son correctas. Para ello, el algoritmo calcula una medida de similitud entre métodos de clustering considerando todos los pares de muestras y los compara contra las verdaderas etiquetas de los clústers. El resultado que arroja ARI está comprendido en un rango de -1 y 1, donde 1 indica coincidencia perfecta, 0 equivale al acuerdo esperado por azar y valores cercanos a -1 reflejan disimilitud. \n",
    "Dado que uno de los objetivos del TFG es promover la diversidad y romper la burbuja de género musical, ARI se interpretará de la siguiente manera:\n",
    "-\tValores altos sugerirán que las agrupaciones reproducen en gran medida las categorías de género tradicionales.\n",
    "-\tValores bajos indicarán que las agrupaciones descubren similitudes transversales entre canciones de distintos géneros.\n",
    "\n",
    "\n",
    "\n",
    "# Bibliografía:\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7695fd3d",
   "metadata": {},
   "source": [
    "<!-- # 3. Fase de modelado\n",
    "\n",
    "\n",
    "## 3.1 Introducción\n",
    "\n",
    "El objetivo de esta fase es abordar tanto el desarrollo como la evaluación del modelo mediante distintas técnicas de aprendizaje no supervisado. Después de llevar a cabo un análisis exploratorio del conjunto de datos y de preparar el dataset con el que se trabajará, se procede a aplicar los algoritmos de *clustering* seleccionados con el fin de identificar patrones de similitud entre canciones basados en sus características musicales. Para cada resultado obtenido con cada uno de los modelo se redactará una breve descripción que permitirá al lector comprender los resultados sin necesidad de dominar los fundamentos básicos de las técnicas del aprendizaje profundo (*deep learning*). Al finalizar la tarea de modelado, se elaborará un análisis exhaustivo sobre el rendimiento de cada modelo para comparar cuál ofrece mejor resultado. \n",
    "\n",
    "## 3.2 Selección de las técnicas de modelado\n",
    "\n",
    "A continuación se propone el conjunto de técnicas de aprendizaje no supervisado que se abordarán en este proyecto:\n",
    "\n",
    "### 3.2.1 K-Means\n",
    "\n",
    "El algoritmo K-Means es un proceso iterativo que busca particionar el conjunto de datos en $k$ grupos con el objetivo minimizar la suma de distancias cuadráticas entre los puntos y los centroides de cada grupo formado. En cada iteración, las referencias de los centroides se actualizan con la media de los puntos asignados hasta que el proceso converge dando lugar a grupos estabilizados.\n",
    "\n",
    "Para establecer el valor $k$, habitualmente se utilizan diferentes técnicas como el método del codo (*Elbow method*), el criterio de la silueta (*Silhoutte Score*) o el índice de Calinski-Harabasz. En este proyecto se decide aplicar las tres técnicas con el objetivo de visualizar **cómo varía la calidad de agrupamiento** en función del número de clústeres y determinar el valor de $k$ que ofrezca el mejor equilibrio entre los grupos.\n",
    "\n",
    "En algoritmo K-Means funciona de la siguiente manera:\n",
    "\n",
    "1. En primer lugar se seleccionan el número de clústeres $k$ utilizando las técnicas anteriores. Que estas técnicas ofrezcan un valor $k$ determinado no implica tener que utilizar únicamente dicho valor, es importante destacar que se pueden generar agrupaciones entorno al valor $k$ que sugieren las técnicas.\n",
    "\n",
    "2. Acto seguido se inicializan las coordenadas de los centroides. Esto se puede realizar de manera aleatoria o simplemente indicando el valor inicial de manera manual. \n",
    "\n",
    "3. Se procede con la asignación de puntos a cada clúster utilizando una métrica de distancia como la euclidiana o la manhattan entre otros.\n",
    "\n",
    "4. Una vez se tienen los puntos del espacio asignados a un clúster determinado, se actualizan los centroides con la siguiente expresión:  \n",
    "    $$\n",
    "    \\mathbf{m}_i^{(t+1)} = \\frac{1}{|S_i^{(t)}|} \\sum_{\\mathbf{x}_j \\in S_i^{(t)}} \\mathbf{x}_j\n",
    "    $$  \n",
    "    donde:\n",
    "    - $\\mathbf{m}_i^{(t+1)}$: El nuevo centroide del clúster $i$ en la iteración $t + 1$\n",
    "    - $|S_i^{(t)}|$: Número de puntos pertenecientes al clúster $i$ en la iteración $t$ \n",
    "    - $x_j$: Representa cada punto del clúster \n",
    "\n",
    "5. Se repiten de manera iterativa los pasos 3 y 4 hasta que el algoritmo converge.\n",
    "\n",
    "Para que el algoritmo converja debe ocurrir que los centroides dejen de cambiar de clúster, llegados a este punto se tienen formado los clústeres que k-Means identifica en el conjunto de datos. \n",
    "\n",
    "\n",
    "### 3.2.2 K-Medoids\n",
    "\n",
    "K-Medoids también conocido como Partitioning Around Medoids (PAM) es un algoritmo de agrupamiento que se utiliza para dividir un conjunto de datos en $k$ grupos. A diferencia de K-Means donde se utiliza el cálculo de medias para establecer los centroides en cada iteración, en K-Medoids se utilizan puntos reales del conjunto de datos bajo el nombre de *medoids*. Este algoritmo es realmente eficiente sobre conjuntos de datos que presentan valores atípicos ya que los centroides se eligen como puntos reales del conjunto de datos en lugar de utilizarse las medias.\n",
    "\n",
    "Para realizar el agrupamiento de los datos a los medoids se utiliza el enfoque de PAM:\n",
    "\n",
    "1. Al igual que ocurre con k-Means se selecciona un valor $k$ utilizando las técnicas expuestas anteriormente.\n",
    "\n",
    "2. Se inicializa aleatoriamente los medoides eligiendo $k$ puntos reales del conjunto de datos.\n",
    "\n",
    "3. Se utiliza una métrica de distancia entre cada dato a los distintos medoides para agruparlo al medoide más cercano.\n",
    "\n",
    "4. Para cada clúster y una vez se realizan todas las asignaciones, se debe comprobar si alguno de los puntos del reduce el coeficiente de disimilitud, es decir, la suma de distancias dentro del grupo. Si se encuentra \n",
    "algún punto que cumpla esta condición se convertirá automáticamente en el nuevo medoide. \n",
    "\n",
    "5. Solamente que un medoide cambie, se realizará el proceso de nuevo.\n",
    "\n",
    "El algoritmo converge cuando todos los medoides no cambian en dos iteraciones seguidas. \n",
    "\n",
    "\n",
    "### 3.2.3 DBSCAN y OPTICS\n",
    "\n",
    "*Density-Based Spatial Clustering of Applications with Noise* (DBSCAN) y *Ordering Points To Identify The Clustering Structure*(OPTICS) son métodos de clustering no supervisado en que la agrupación está basada en la densidad de los datos. El método DBSCAN permite identificar patrones en un juego de datos agrupando los puntos que están más cercanos a alguna métrica. El método recibe dos parámetros, eps y minPoints:\n",
    "\n",
    "- epsilon (eps) : distancia máxima entre dos puntos para que uno sea considerado vecino del otro\n",
    "\n",
    "- minPoints: Establece el mínimo de puntos para que el método lo considere un clúster\n",
    "\n",
    "DBSCAN tiene la limitación de que trabaja con agrupación de densidades de datos fijas. El método OPTICS, podría considerarse una extensión de DBSCAN que ofrece la oportunidad de trabajar con agrupación de densidades variables. Además, el parámetro epsilon en OPTICS no determina la formación de clústeres sino que sirve para disminuir la complejidad de cálculo para el algoritmo.\n",
    "\n",
    "\n",
    " -->\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
